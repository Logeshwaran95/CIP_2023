{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6a7116",
   "metadata": {},
   "source": [
    "<center><h2>PREPROCESSING</h2></center>\n",
    "<p>\n",
    "    1. Importing the necessary libraries and Downloading the necessary packages\n",
    "    <br>\n",
    "    2. Creating a DataFrame : The DataFrame has 3 rows and 5 columns.\n",
    "    <br>\n",
    "    3. Defining the stop words\n",
    "    <br>\n",
    "    \n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716f2d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Logesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Logesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Logesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Logesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from os import walk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = pd.DataFrame(np.random.choice(10, (3, 5)), columns=list('ABCDE'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ea7c9",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <h3>Defining a function to check for hypernyms: </h3>\n",
    "    \n",
    "    The code defines a function called check_for_hypernim() which takes a token as input and returns its hypernyms. The function uses the WordNet corpus from nltk.corpus to find the hypernyms of the token.\n",
    "    \n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856dec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def check_for_hypernim(token):\n",
    "    hypernims = []\n",
    "    for i in range(15):\n",
    "        try:\n",
    "            hypernims1 = []\n",
    "            for i, j in enumerate(wn.synsets(token)):\n",
    "                for l in j.hypernyms():\n",
    "                    hypernims1.append(l.lemma_names()[0])\n",
    "            token = hypernims1[0]\n",
    "            hypernims.append(hypernims1)\n",
    "        except IndexError:\n",
    "            hypernims.append([token])\n",
    "\n",
    "    return hypernims\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db87f3b",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <h3>Reading the input file:</h3>\n",
    "    \n",
    "    The code reads a CSV file called \"sample_input.csv\" and extracts the words from it.\n",
    "    \n",
    "<h3>Filtering out stop words: </h3>\n",
    "    \n",
    "    The code filters out the stop words from the extracted words.\n",
    "    \n",
    "<h3>Part of speech tagging: </h3>\n",
    "    \n",
    "    The code performs part-of-speech (POS) tagging on the filtered words using nltk.pos_tag() function.\n",
    "    \n",
    "<h3>Filtering out unnecessary parts of speech: </h3>\n",
    "    \n",
    "    The code filters out certain parts of speech that are unlikely to have hypernyms, such as determiners, conjunctions, interjections, and prepositions.\n",
    "<h3>\n",
    "Writing hypernyms to a CSV file: </h3>\n",
    "\n",
    "\n",
    "    The code writes the hypernyms of each word to a new CSV file called \"sample_hypernyms.csv\". It uses the check_for_hypernim() function to find the hypernyms of each word and writes them to the CSV file.\n",
    "\n",
    "<h3>Inputs:</h3>\n",
    "\n",
    "\n",
    "1. \"data/sample_input.csv\": A CSV file containing text data.\n",
    "    \n",
    "2. English stop words: A set of predefined stop words in the English language.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1aef5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"data/sample_input.csv\",encoding='latin1')\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "with open(\"results/sample_hypernyms.csv\", 'w',encoding='latin1') as csv_file:\n",
    "    filewriter = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    filtered_sentence = []\n",
    "\n",
    "    x = []\n",
    "    for r in words:\n",
    "\n",
    "        if not r in stop_words:\n",
    "            filtered_sentence.append(r)\n",
    "    tagged = nltk.pos_tag(filtered_sentence)\n",
    "    for i in tagged:\n",
    "        if len(i[0]) != 0 or len(i[0]) != 1:\n",
    "            if i[1] == 'IN' or i[1] == 'DT' or i[1] == 'CD' or i[1] == 'CC' or i[1] == 'EX' or i[1] == 'MD' or   i[1] == 'WDT' or i[1] == 'WP' or i[1] == 'UH' or i[1] == 'TO' or i[1] == 'RP' or i[1] == 'PDT' or i[1] == 'PRP' or i[1] == 'PRP$' or i[0] == 'co':\n",
    "                # print(i[0])\n",
    "                continue\n",
    "            else:\n",
    "                x.append(i[0].rstrip(\".,?!\"))\n",
    "    for i in x:\n",
    "        l = []\n",
    "        l.append(i)\n",
    "        hype = check_for_hypernim(i)\n",
    "        if len(hype) == 0:\n",
    "            hype.append(i)  # 1\n",
    "            hype.append(i)  # 2\n",
    "            hype.append(i)  # 3\n",
    "            hype.append(i)  # 4\n",
    "            hype.append(i)  # 5\n",
    "            hype.append(i)  # 6\n",
    "            hype.append(i)  # 7\n",
    "            hype.append(i)  # 8\n",
    "            hype.append(i)  # 9\n",
    "            hype.append(i)  # 10\n",
    "            hype.append(i)  # 11\n",
    "            hype.append(i)  # 12\n",
    "            hype.append(i)  # 13\n",
    "            hype.append(i)  # 14\n",
    "            hype.append(i)  # 15\n",
    "        for hyper in hype:\n",
    "            l.append(hyper[0])\n",
    "        filewriter.writerow(l)\n",
    "    csv_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f8497",
   "metadata": {},
   "source": [
    "<p>\n",
    "    \n",
    "<h3>Reading hypernyms from the CSV file: </h3>\n",
    "    \n",
    "    The code reads the \"sample_hypernyms.csv\" file and prints the hypernyms for the first 5 tokens in the file.\n",
    "\n",
    "<h3>Outputs:</h3>\n",
    "\n",
    "\n",
    "1. \"results/sample_hypernyms.csv\": A CSV file containing the hypernyms of each word from the input file.\n",
    "    \n",
    "2. Console output: The hypernyms of the first 5 tokens from the \"sample_hypernyms.csv\" file.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c458ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:    12,jack,2020-06-09\n",
      "Hypernyms: 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09, 12,jack,2020-06-09\n",
      "\n",
      "Token:    https://t.co/cF8GJ2CidY\\n\\nAnd\n",
      "Hypernyms: https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd, https://t.co/cF8GJ2CidY\\n\\nAnd\n",
      "\n",
      "Token:    you\\xe2\\x80\\x99re\n",
      "Hypernyms: you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re, you\\xe2\\x80\\x99re\n",
      "\n",
      "Token:    part\n",
      "Hypernyms: relation, abstraction, concept, idea, content, collection, group, abstraction, concept, idea, content, collection, group, abstraction, concept\n",
      "\n",
      "Token:    company\n",
      "Hypernyms: institution, organization, social_group, group, abstraction, concept, idea, content, collection, group, abstraction, concept, idea, content, collection\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"results/sample_hypernyms.csv\", 'r', encoding='latin1') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    n = 5  # number of rows to print\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if i >= n:\n",
    "            break\n",
    "        token = row[0]\n",
    "        hypernyms = row[1:]\n",
    "        print(\"Token:   \", token)\n",
    "        print(\"Hypernyms:\", \", \".join(hypernyms))\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

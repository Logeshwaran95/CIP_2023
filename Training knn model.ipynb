{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d1f98d",
   "metadata": {},
   "source": [
    "# Importing necessary libraries and modules\n",
    "\n",
    "\n",
    "\n",
    "This block imports the libraries and modules required for data processing and others.\n",
    "\n",
    "\n",
    "* listdir and isfile from the os module\n",
    "* csv\n",
    "* matplotlib\n",
    "* numpy\n",
    "* pandas\n",
    "* sklearn (for machine learning)\n",
    "* Word2Vec from the gensim module.\n",
    "\n",
    "<p>\n",
    "Initializing data structures: This block initializes two empty lists rowsx and yx.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3616918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import scale\n",
    "from collections import Counter\n",
    "rowsx = []\n",
    "yx = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a1446",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h2>Reading data from the training dataset:</h2> \n",
    "\n",
    "* This block opens the train.csv file and reads each row. \n",
    "* It extracts the first column as yx and the rest of the columns as rows1. \n",
    "* It splits each column on the newline character (\\n) and appends the resulting strings to rows1.\n",
    "* It then appends rows1 to rowsx.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fb8b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/train.csv\", 'r', encoding='latin1') as csv1:\n",
    "    # creating a csv reader object\n",
    "    csvreader1 = csv.reader(csv1)\n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader1:\n",
    "        rows1 = []\n",
    "        for i in range(1, len(row)-15):\n",
    "            for j in row[i].split(\"\\n\"):\n",
    "                    rows1.append(j)\n",
    "        yx.append(row[0])\n",
    "        del (row[0])\n",
    "        rowsx.append(rows1)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c5ee2",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings: \n",
    "\n",
    "* This block uses the Word2Vec class from the gensim module to create word embeddings for each sentence in rowsx.\n",
    "* It sets the embedding dimension to 200 and the minimum count to 3, and trains the embeddings using the sentences in rowsx.\n",
    "\n",
    "# TF-IDF Vectorization: \n",
    "\n",
    "* This block uses the TfidfVectorizer class from the sklearn module to generate a TF-IDF (term frequency-inverse document frequency) matrix for the sentences in rowsx.\n",
    "* It sets the minimum document frequency to 3 and fits the vectorizer to the sentences in rowsx. \n",
    "* It also creates a dictionary tfidf_map that maps each word to its corresponding IDF value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4881d280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logesh\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "embeddings = Word2Vec(vector_size=200, min_count=3)\n",
    "embeddings.build_vocab([sentence for sentence in rowsx])\n",
    "embeddings.train([sentence for sentence in rowsx],\n",
    "                 total_examples=embeddings.corpus_count,\n",
    "                 epochs=embeddings.epochs)\n",
    "\n",
    "gen_tfidf = TfidfVectorizer(analyzer=lambda x: x, min_df=3)\n",
    "matrix = gen_tfidf.fit_transform([sentence   for sentence in rowsx])\n",
    "tfidf_map = dict(zip(gen_tfidf.get_feature_names(), gen_tfidf.idf_))\n",
    "\n",
    "print(len(tfidf_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed64c82",
   "metadata": {},
   "source": [
    "# Sentence Encoding: \n",
    "\n",
    "* This block defines a function encode_sentence that takes in a list of tokens (words) and the embedding size (200 in this case), and encodes the sentence using the Word2Vec embeddings and the TF-IDF values for each word.\n",
    "* If a word is not present in the embeddings, it is ignored. \n",
    "* The function returns a vector representation of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4101fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(tokens, emb_size):\n",
    "    _vector = np.zeros((1, emb_size))\n",
    "    length = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            _vector += embeddings.wv[word].reshape((1, emb_size)) * tfidf_map[word]\n",
    "            length += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    if length > 0:\n",
    "        _vector /= length\n",
    "\n",
    "    return _vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02dd7d8",
   "metadata": {},
   "source": [
    "# Encoding the Training Data: \n",
    "\n",
    "\n",
    "* This block encodes each sentence in rowsx using the encode_sentence function and concatenates the resulting vectors into a single matrix x_train.\n",
    "* It then standardizes the matrix using the scale function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a9772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(474824, 200)\n"
     ]
    }
   ],
   "source": [
    "x_train = scale(np.concatenate([encode_sentence(ele, 200) for ele in map(lambda x: x, rowsx)]))\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fedc1",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Classifier:\n",
    "\n",
    "* This block creates a KNN classifier using the KNeighborsClassifier class from sklearn, with n_neighbors set to 30. \n",
    "* It trains the classifier on the standardized training data (x_train) and the corresponding labels (yx), and prints the message \"done\" when the operation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7a4781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "modelknn = KNeighborsClassifier(n_neighbors=30)\n",
    "modelknn.fit(x_train,yx)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61da3f",
   "metadata": {},
   "source": [
    "# Reading data from the test dataset: \n",
    "\n",
    "This block reads the sample_test.csv file and processes it in the same way as the training data (excluding the first column, which contains the labels).\n",
    "\n",
    "# Encoding the Test Data: \n",
    "\n",
    "This block encodes each sentence in rowsx1 using the encode_sentence function and concatenates the resulting vectors into a single matrix x_test. It then standardizes the matrix using the scale function from sklearn.\n",
    "\n",
    "# Predicting labels:\n",
    "\n",
    "* This block predicts class labels for test data using KNN, counts the number of predictions for each class label, and sorts the counts in descending order.\n",
    "* It stores the predicted labels and count of each label in predicted_labels_knn and counts respectively. \n",
    "* The sorted list of counts, including the label and count, is stored in listcount.\n",
    "* Finally, it prints the counts dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484c524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'2391': 29, '1026541': 10, '2554': 7, '772386': 7, '12189': 6, '1159221': 5, '27778221': 5, '42902132': 5, '10350': 5, '10297': 5, '3359851': 4, '1075': 4, '14227633': 4, '12111': 4, '1273081': 4, '12211342': 4, '985': 4, '611823': 4, '1520661': 3, '10021642': 3, '246': 3, '39501385': 3, '48763': 3, '1373201': 3, '14080395': 3, '730073': 3, '3840': 3, '3011091': 3, '50518441': 3, '6327412': 3, '10487982': 3, '12389252': 3, '3452911': 3, '12765532': 3, '40956484': 3, '11470302': 3, '14237044': 3, '1814': 3, '784492': 3, '14072428': 3, '4737': 3, '235': 3, '14105596': 3, '12336822': 3, '813286': 3, '13050': 3, '11994112': 2, '30332372': 2, '1285021': 2, '14522707': 2, '62763': 2, '5434332': 2, '1797691': 2, '3043001': 2, '11537': 2, '1119751': 2, '6144362': 2, '1621271': 2, '365': 2, '1469': 2, '14052380': 2, '2053': 2, '1618221': 2, '1085': 2, '8291': 2, '11078052': 2, '5307': 2, '2774': 2, '24053': 2, '31253': 2, '10283882': 2, '54433665': 2, '44539471': 2, '1319451': 2, '1668': 2, '801234': 2, '14123175': 2, '5765': 2, '1186': 2, '33278083': 2, '1478981': 2, '12362552': 2, '5768872': 2, '761731': 2, '56': 2, '67': 2, '336113': 2, '10312': 2, '14089289': 2, '800865': 2, '10173822': 2, '2167': 2, '14081803': 2, '78733': 2, '238': 2, '4291171': 2, '14068094': 2, '712163': 2, '1416671': 2, '12093512': 2, '12335': 2, '11069462': 2, '338': 2, '79053': 2, '27894178': 2, '12469782': 2, '13407': 2, '14049014': 2, '3804': 2, '1143031': 1, '1692901': 1, '40890705': 1, '3609561': 1, '734493': 1, '12305': 1, '983291': 1, '963111': 1, '12662472': 1, '1105211': 1, '665403': 1, '28260558': 1, '13992552': 1, '1264711': 1, '23929990': 1, '801709': 1, '28382257': 1, '678093': 1, '4078411': 1, '800273': 1, '14166387': 1, '649913': 1, '309073': 1, '14047249': 1, '2039261': 1, '4355421': 1, '3767541': 1, '1173': 1, '12298152': 1, '709143': 1, '28063180': 1, '60297902': 1, '5082531': 1, '57410728': 1, '12439322': 1, '10997': 1, '16310463': 1, '14062005': 1, '12041': 1, '12215002': 1, '662783': 1, '1333081': 1, '10997032': 1, '1593581': 1, '754468': 1, '3226': 1, '922101': 1, '5167141': 1, '12223582': 1, '1941': 1, '782575': 1, '1262421': 1, '27345820': 1, '27117081': 1, '2148071': 1, '12292752': 1, '1019161': 1, '1175221': 1, '126': 1, '1050111': 1, '609023': 1, '598083': 1, '194': 1, '324': 1, '150': 1, '42614637': 1, '41393378': 1, '29491850': 1, '41173460': 1, '6106': 1, '607063': 1, '668473': 1, '4212351': 1, '63793': 1, '43417156': 1, '1002351': 1, '1630031': 1, '14049060': 1, '12127422': 1, '5774792': 1, '751163': 1, '299': 1, '1761301': 1, '2575461': 1, '1073501': 1, '697453': 1, '1435351': 1, '14200687': 1, '12597762': 1, '51918003': 1, '2383761': 1, '800502': 1, '28613': 1, '801390': 1, '12074812': 1, '10706': 1, '717313': 1, '2703021': 1, '10262282': 1, '5711': 1, '15383': 1, '6151922': 1, '1785871': 1, '14107456': 1, '1017181': 1, '1253511': 1, '632403': 1, '27643332': 1, '801202': 1, '800316': 1, '5151': 1, '6081892': 1, '801104': 1, '1239741': 1, '4107251': 1, '801521': 1, '661553': 1, '21956680': 1, '784304': 1, '1169421': 1, '801345': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logesh\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "rowsx1 = []\n",
    "with open(\"./data/sample_test.csv\", 'r', encoding='latin1') as csv1:\n",
    "    csvreader1 = csv.reader(csv1)\n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader1:\n",
    "        rows1 = []\n",
    "        for i in range(1, len(row)-15):\n",
    "            for j in row[i].split(\"\\n\"):\n",
    "                rows1.append(j)\n",
    "        del (row[0])\n",
    "        rowsx1.append(rows1)\n",
    "x_test = scale(np.concatenate([encode_sentence(ele, 200) for ele in map(lambda x: x, rowsx1)]))\n",
    "predicted_labels_knn = modelknn.predict(x_test)\n",
    "counts = Counter(predicted_labels_knn)\n",
    "listcount = [(l, k) for k, l in sorted([(j, i) for i, j in counts.items()], reverse=True)]\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
